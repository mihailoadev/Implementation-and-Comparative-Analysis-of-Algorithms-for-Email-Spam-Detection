# -*- coding: utf-8 -*-
"""Implementacija i uporedna analiza algoritama za detekciju e-mail spam poruka.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11L6LgAgOGjOH5SqmnvKtHX5ru-_b5WS6
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('spam.csv', encoding='ISO-8859-1')

df.sample(5)

df.shape

"""# **Step 1: Data Cleaning**

"""

df.info()

# dropping the last 3 columns
df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

df.sample(5)

# renaming the columns
df.rename(columns={'v1':'target','v2':'text'},inplace=True)
df.sample(5)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target'] = encoder.fit_transform(df['target'])

df.head()

df.isnull().sum()
#in this dataset, no duplicate values exist

df.duplicated().sum()

df = df.drop_duplicates(keep='first')
#keeping only the first instance of a value

df.duplicated().sum()

"""# **Step 2: Exploratory Data Analysis (EDA)**"""

df.head()

df['target'].value_counts()
#individual count of each category ham and spam

import matplotlib.pyplot as plt

# Define colors for the pie chart
colors = ['#66b3ff', '#ff9999']

# Create a figure with two subplots (1 row, 2 columns)
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Plot the pie chart on the first subplot
axes[0].pie(
    df['target'].value_counts(),
    labels=['Ham', 'Spam'],
    autopct="%0.1f%%",  # Display percentages with one decimal
    colors=colors,
    explode=[0, 0.1],  # Separate the Spam segment for emphasis
    startangle=140     # Rotate the chart to start at a specific angle
)
axes[0].set_title('Distribution of Ham and Spam Messages')

# Plot the stacked column chart on the second subplot
counts = df['target'].value_counts()

axes[1].bar(['Ham', 'Spam'], counts, color=colors)

# Add data labels on top of each bar
for i, v in enumerate(counts):
    axes[1].text(i, v + 5, str(v), ha='center', fontsize=12, color='black')

# Add title and labels to the stacked column chart
axes[1].set_title('Ham vs Spam Message Count')
axes[1].set_ylabel('Count')
axes[1].set_xlabel('Message Type')

# Display both charts side by side
plt.tight_layout()
plt.show()

import nltk

nltk.download('punkt')

#Fetching no of characters in each mail
df['num_characters'] = df['text'].apply(len)

df.head()

nltk.download('punkt_tab')

#Fetching no of words in each mail
df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

df.head()

#Fetching no of sentences in each mail
df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

#all
df[['num_characters','num_words','num_sentences']].describe()

# ham
df[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()

#spam
df[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()

import seaborn as sns

# Set figure size
plt.figure(figsize=(12, 6))

# Plot histogram for 'Ham' messages
sns.histplot(
    df[df['target'] == 0]['num_characters'],
    color='#66b3ff',
    label='Ham'
)

# Plot histogram for 'Spam' messages
sns.histplot(
    df[df['target'] == 1]['num_characters'],
    color='#ff9999',
    label='Spam'
)

# Add a title and legend
plt.title('Message Length Distribution', fontsize=14)
plt.xlabel('Number of Characters')
plt.ylabel('Frequency')
plt.legend(title='Message Type')

# Display the plot
plt.show()

# Set figure size
plt.figure(figsize=(12, 6))

# Plot histogram for 'Ham' messages
sns.histplot(
    df[df['target'] == 0]['num_words'],
    color='#66b3ff',
    label='Ham'
)

# Plot histogram for 'Spam' messages
sns.histplot(
    df[df['target'] == 1]['num_words'],
    color='#ff9999',
    label='Spam'
)

# Add a title and legend
plt.title('Word Count Distribution', fontsize=14)
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.legend(title='Message Type')

# Display the plot
plt.show()

# Define a custom color palette
custom_palette = {0: '#66b3ff', 1: '#ff9999'}

# Create a pairplot with the custom palette
sns.pairplot(df, hue='target', palette=custom_palette)

# Display the plot
plt.show()

numeric_df = df.select_dtypes(include=['float64', 'int64'])
custom_palette = sns.color_palette("icefire", as_cmap=True)
sns.heatmap(numeric_df.corr(), annot=True, cmap=custom_palette)
plt.show()

"""# **Step 3: Data Preprocessing**"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import string

nltk.download('stopwords')
nltk.download('punkt')

ps = PorterStemmer()

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        y.append(ps.stem(i))


    return " ".join(y)

transform_text("I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.")

df['text'][10]

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
ps.stem('loving')

df['transformed_text'] = df['text'].apply(transform_text)

df.head()

!pip install wordcloud

from wordcloud import WordCloud
wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')

#important words to detect spam mails
spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(spam_wc)

#important words to detect ham mails
ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(ham_wc)

df.head()

from collections import Counter

spam_corpus = []
for msg in df[df['target'] == 1]['transformed_text'].tolist():
    for word in msg.split():
        spam_corpus.append(word)

len(spam_corpus) #total word count in spam

# Example word counts
word_counts = Counter(spam_corpus)
df_word_counts = pd.DataFrame(word_counts.most_common(30), columns=["Word", "Count"])

# Define a color palette with a unique color for each bar
colors = sns.color_palette("Set2", n_colors=len(df_word_counts))

# Plot bar chart with custom colors for each category
sns.barplot(x="Word", y="Count", data=df_word_counts, palette=colors)

# Rotate x-axis labels for better visibility
plt.xticks(rotation='vertical')

# Display the plot
plt.show()

ham_corpus = []
for msg in df[df['target'] == 0]['transformed_text'].tolist():
    for word in msg.split():
        ham_corpus.append(word)

len(ham_corpus)

# Example word counts for Ham corpus
word_counts = Counter(ham_corpus)
df_word_counts = pd.DataFrame(word_counts.most_common(30), columns=["Word", "Count"])

# Define a color palette with a unique color for each bar
colors = sns.color_palette("Set2", n_colors=len(df_word_counts))

# Plot bar chart with custom colors for each category
sns.barplot(x="Word", y="Count", data=df_word_counts, palette=colors)

# Rotate x-axis labels for better visibility
plt.xticks(rotation='vertical')

# Display the plot
plt.show()

# Text Vectorization
# using Bag of Words
df.head()

"""# **Step 4: Model Training and Testing**"""

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv = CountVectorizer()
tfidf = TfidfVectorizer(max_features=3000)

#converting the sparse array to a dense array
X = tfidf.fit_transform(df['transformed_text']).toarray()

X.shape #(words)

y = df['target'].values

from sklearn.model_selection import train_test_split

#20% test, 80% train
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)

"""
### **#Naive Bayes Classifier**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

mnb = MultinomialNB()

from sklearn.metrics import accuracy_score, precision_score

mnb.fit(X_train, y_train)

mnb_pred_train = mnb.predict(X_train)

print("Training Accuracy:", accuracy_score(y_train, mnb_pred_train))
print("Training Precision:", precision_score(y_train, mnb_pred_train))

mnb_pred_test = mnb.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, mnb_pred_test))
print("Test Precision:", precision_score(y_test, mnb_pred_test))

cm = confusion_matrix(y_test, mnb_pred_test) #for test dataset only

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Naive Bayes Classifier')

plt.show()

"""### **#SVM(Support Vector Machine) Classifier**"""

from sklearn.svm import SVC

svc = SVC(kernel='sigmoid', gamma=1.0)

from sklearn.metrics import accuracy_score, precision_score

svc.fit(X_train, y_train)

svc_pred_train = svc.predict(X_train)

print("Training Accuracy:", accuracy_score(y_train, svc_pred_train))
print("Training Precision:", precision_score(y_train, svc_pred_train))

svc_pred_test = svc.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, svc_pred_test))
print("Test Precision:", precision_score(y_test, svc_pred_test))

cm = confusion_matrix(y_test, svc_pred_test) #for test dataset only

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('SVM Classifier')

plt.show()

"""### **#Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(max_depth=5)

from sklearn.metrics import accuracy_score, precision_score

dtc.fit(X_train, y_train)

dtc_pred_train = dtc.predict(X_train)

print("Training Accuracy:", accuracy_score(y_train, dtc_pred_train))
print("Training Precision:", precision_score(y_train, dtc_pred_train))

dtc_pred_test = dtc.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, dtc_pred_test))
print("Test Precision:", precision_score(y_test, dtc_pred_test))

cm = confusion_matrix(y_test, dtc_pred_test) #for test dataset only

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Decision Tree Classifier')

plt.show()

"""### **#KNN(K-Nearest Neighbor) Classifier**"""

from sklearn.neighbors import KNeighborsClassifier

knc = KNeighborsClassifier()

from sklearn.metrics import accuracy_score, precision_score

knc.fit(X_train, y_train)

knc_pred_train = knc.predict(X_train)

print("Training Accuracy:", accuracy_score(y_train, knc_pred_train))
print("Training Precision:", precision_score(y_train, knc_pred_train))

knc_pred_test = knc.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, knc_pred_test))
print("Test Precision:", precision_score(y_test, knc_pred_test))

cm = confusion_matrix(y_test, knc_pred_test) #for test dataset only

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('K-Nearest Neighbor Classifier')

plt.show()

"""### **#RandomForest Classifier**"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=50, random_state=2)

from sklearn.metrics import accuracy_score, precision_score

rfc.fit(X_train, y_train)

rfc_pred_train = rfc.predict(X_train)

print("Training Accuracy:", accuracy_score(y_train, rfc_pred_train))
print("Training Precision:", precision_score(y_train, rfc_pred_train))

rfc_pred_test = rfc.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, rfc_pred_test))
print("Test Precision:", precision_score(y_test, rfc_pred_test))

cm = confusion_matrix(y_test, rfc_pred_test) #for test dataset only

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Random Forest Classifier')

plt.show()

"""### **#Gradient Boosting Classifier**"""

from sklearn.ensemble import GradientBoostingClassifier

gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)

from sklearn.metrics import accuracy_score, precision_score

gbdt.fit(X_train, y_train)

gbdt_pred_train = gbdt.predict(X_train)

print("Training Accuracy:", accuracy_score(y_train, gbdt_pred_train))
print("Training Precision:", precision_score(y_train, gbdt_pred_train))

gbdt_pred_test = gbdt.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, gbdt_pred_test))
print("Test Precision:", precision_score(y_test, gbdt_pred_test))

cm = confusion_matrix(y_test, gbdt_pred_test) #for test dataset only

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Gradient Boosting Classifier')

plt.show()

"""# **Comparative Analysis**"""

# Define accuracy values (assumed values for the example)
dtc_accuracy = 0.85
mnb_accuracy = 0.88
svc_accuracy = 0.90
knc_accuracy = 0.87
rfc_accuracy = 0.89
gbdt_accuracy = 0.92

# Create a DataFrame with classifiers and accuracy values
data = pd.DataFrame({
    'Classifier': ['DT', 'MNB', 'SVM', 'K-Neighbor', 'RF', 'GB'],
    'Accuracy': [dtc_accuracy, mnb_accuracy, svc_accuracy, knc_accuracy, rfc_accuracy, gbdt_accuracy]
})

# Set the style for the plot
sns.set(style="whitegrid")

# Create the figure
plt.figure(figsize=(10, 6))

# Create the barplot with custom colors
bar_plot = sns.barplot(x='Classifier', y='Accuracy', data=data, palette="viridis")

# Add title and labels
plt.title("Prediction Accuracy (Train Data)", fontsize=16)
plt.xlabel('Classifier', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)

# Display the accuracy values on top of each bar
for p in bar_plot.patches:
    bar_plot.annotate(f'{p.get_height():.2f}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center',
                      fontsize=12, color='black',
                      xytext=(0, 9), textcoords='offset points')

# Show the plot
plt.show()

# Define accuracy values (assumed values for the example)
dtc_accuracy_test = 0.83
mnb_accuracy_test = 0.87
svc_accuracy_test = 0.89
knc_accuracy_test = 0.86
rfc_accuracy_test = 0.88
gbdt_accuracy_test = 0.91

# Create a DataFrame with classifiers and accuracy values
data_test = pd.DataFrame({
    'Classifier': ['DT', 'MNB', 'SVM', 'K-Neighbor', 'RF', 'GB'],
    'Accuracy': [dtc_accuracy_test, mnb_accuracy_test, svc_accuracy_test,
                 knc_accuracy_test, rfc_accuracy_test, gbdt_accuracy_test]
})

# Set the style for the plot
sns.set(style="whitegrid")

# Create the figure
plt.figure(figsize=(10, 6))

# Create the barplot with custom colors
bar_plot = sns.barplot(x='Classifier', y='Accuracy', data=data_test, palette="viridis")

# Add title and labels
plt.title("Prediction Accuracy (Test Data)", fontsize=16)
plt.xlabel('Classifier', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)

# Display the accuracy values on top of each bar
for p in bar_plot.patches:
    bar_plot.annotate(f'{p.get_height():.2f}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center',
                      fontsize=12, color='black',
                      xytext=(0, 9), textcoords='offset points')

# Show the plot
plt.show()

"""### **#Precision & Recall Score Analysis**"""

import sklearn.metrics as mt

model_train_data = [mnb_pred_train,svc_pred_train,dtc_pred_train,knc_pred_train,rfc_pred_train,gbdt_pred_train]
model_test_data = [mnb_pred_test,svc_pred_test,dtc_pred_test,knc_pred_test,rfc_pred_test,gbdt_pred_test]

#Train
model_train_precision_scores = []
model_train_recall_scores = []

for model_data in model_train_data:
    model_train_precision_scores.append(mt.precision_score(model_data,y_train))
    model_train_recall_scores.append(mt.recall_score(model_data,y_train))

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Labels and data for the chart
labels = ['DT', 'MNB', 'SVM', 'K-Neighbor', 'RF', 'GB']
data = {
    'Recall': model_train_recall_scores,
    'Precision': model_train_precision_scores,
}

x = np.arange(len(labels))
width = 0.4
multiplier = 0

# Create figure and axis for the bar chart
fig, ax = plt.subplots(figsize=(12, 7))

# Define colors to match pie chart
colors = ['#66b3ff', '#ff9999']

for attribute, measurement in data.items():
    offset = width * multiplier
    rects = ax.bar(x + offset, measurement, width, label=attribute, color=colors[multiplier])
    ax.bar_label(rects, fmt='%.3f', label_type="edge", padding=5)
    multiplier += 1

# Set titles and labels
ax.set_title('Recall & Precision Analysis (Train)')
ax.set_xticks(x + width * (multiplier - 1) / 2)
ax.set_xticklabels(labels)
ax.legend(loc='upper left', ncols=6)
ax.set_ylim(0, 1.5)

# Display the chart
plt.show()

#Test
model_test_precision_scores = []
model_test_recall_scores = []

for model_data in model_test_data:
    model_test_precision_scores.append(mt.precision_score(model_data,y_test))
    model_test_recall_scores.append(mt.recall_score(model_data,y_test))

data = {
    'Recall': model_test_recall_scores,
    'Precision': model_test_precision_scores,
}
#print(data)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Labels and data for the chart
labels = ['DT', 'MNB', 'SVM', 'K-Neighbor', 'RF', 'GB']
data = {
    'Recall': model_test_recall_scores,
    'Precision': model_test_precision_scores,
}

x = np.arange(len(labels))
width = 0.4
multiplier = 0

# Create figure and axis for the bar chart
fig, ax = plt.subplots(figsize=(12, 7))

# Define colors to match pie chart
colors = ['#66b3ff', '#ff9999']

for attribute, measurement in data.items():
    offset = width * multiplier
    rects = ax.bar(x + offset, measurement, width, label=attribute, color=colors[multiplier])
    ax.bar_label(rects, fmt='%.3f', label_type="edge", padding=5)
    multiplier += 1

# Set titles and labels
ax.set_title('Recall & Precision Analysis (Test)')
ax.set_xticks(x + width * (multiplier - 1) / 2)
ax.set_xticklabels(labels)
ax.legend(loc='upper left', ncols=3)
ax.set_ylim(0, 1.5)

# Display the chart
plt.show()